{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsbIMdfKCK1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow==1.7.*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbpOBC1ot3U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import json\n",
        "stemmer = LancasterStemmer()\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDchbFpArBYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tflearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFKr3Zv5CNeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROUYll2drIzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tflearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kfxtJr_Cvj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8l_7SEPEGph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd My\\ Drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbCx_oieHfAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL2UWWF6EJZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import entities\n",
        "import preprocessing\n",
        "import json\n",
        "intents_file = \"intents.json\"\n",
        "with open(intents_file) as json_data:\n",
        "    intents = json.load(json_data)\n",
        "    words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "nltk.download('punkt')\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEG4XaLbFNxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "classes = sorted(list(set(classes)))\n",
        "print (len(documents), \"Sentences for training\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P53w8TgFm44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = []\n",
        "output = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoGZcsThFqvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_empty = [0] * len(classes)\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "    # training.append([bag, output_row])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg_BSOWmpo7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "# create train and test lists\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAS06UjOo0tH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdoo2K2q1jt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tflearn\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "# model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "# # load our saved model\n",
        "# model.load('./model.tflearn')\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save('model.tflearn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2D_rkL41ya2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle.dump({'words': words, 'classes': classes, 'train_x': train_x, 'train_y': train_y}, open(\"training_data\", \"wb\"))\n",
        "import pickle\n",
        "data = pickle.load(open(\"training_data\", \"rb\"))\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "import json\n",
        "with open(intents_file) as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLSKou7z2N86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessing\n",
        "# load our saved model\n",
        "model.load('./model.tflearn')\n",
        "def bow(sentence, words, show_details=False):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = preprocessing.standardize_sentence2(sentence)\n",
        "    # bag of words\n",
        "    bag = [0] * len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return (np.array(bag))\n",
        "\n",
        "ERROR_THRESHOLD = 0.5\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "import re\n",
        "def get_words(sentence,tag):\n",
        "  words=[]\n",
        "  str1= sentence;\n",
        "  str1=str1.lower(); \n",
        "  if (tag==\"Instate\"):\n",
        "    if(\"kerala\" in str1):\n",
        "      words.append(\"Kerala\")\n",
        "    if(\"maharastra\" in str1):\n",
        "      words.append(\"Maharastra\")\n",
        "    if(\"tamil\" in str1):\n",
        "      words.append(\"Tamil\")\n",
        "    if(\"uttar\" in str1):\n",
        "      words.append(\"Uttar Pradesh\")\n",
        "    if(\"karnataka\" in str1):\n",
        "      words.append(\"karnataka\")\n",
        "    if(\"gujarat\" in str1):\n",
        "      words.append(\"Gujarat\")\n",
        "    if(\"punjab\" in str1):\n",
        "      words.append(\"Punjab\")\n",
        "    if(\"rajastan\" in str1):\n",
        "      words.append(\"Rajastan\")\n",
        "    if(\"jammu\" in str1):\n",
        "      words.append(\"Jammu\")\n",
        "    if(\"kashmir\" in str1):\n",
        "      words.append(\"Kashmir\")\n",
        "    if(\"bihar\" in str1):\n",
        "      words.append(\"Bihar\")\n",
        "    if(\"telangana\" in str1):\n",
        "      words.append(\"Telangana\")\n",
        "    if(\"bengal\" in str1):\n",
        "      words.append(\"West bengal\")\n",
        "    if(\"andhra\" in str1):\n",
        "      words.append(\"Andhra Pradesh\")\n",
        "    if(\"madhya\" in str1):\n",
        "      words.append(\"Madhya Pradesh\")\n",
        "    if(\"odisha\" in str1):\n",
        "      words.append(\"Odisha\")\n",
        "    if(\"assam\" in str1):\n",
        "      words.append(\"Assam\")\n",
        "    if(\"haryana\" in str1):\n",
        "      words.append(\"Haryana\")\n",
        "    if(\"jharkhand\" in str1):\n",
        "      words.append(\"Jharkhand\")\n",
        "    if(\"goa\" in str1):\n",
        "      words.append(\"Goa\")\n",
        "    if(\"manipur\" in str1):\n",
        "      words.append(\"Manipur\")\n",
        "    if(\"himachal\" in str1):\n",
        "      words.append(\"Himachal Pradesh\")\n",
        "    if(\"nagaland\" in str1):\n",
        "      words.append(\"Nagaland\")\n",
        "    if(\"arunachal\" in str1):\n",
        "      words.append(\"Arunachal Pradesh\")\n",
        "    if(\"sikkim\" in str1):\n",
        "      words.append(\"Sikkim\")  \n",
        "    if(\"chhattisgarh\" in str1):\n",
        "      words.append(\"Chhattisgarh\")\n",
        "    if(\"uttarakhand\" in str1):\n",
        "      words.append(\"Uttarakhand\")\n",
        "    if(\"meghalaya\" in str1):\n",
        "      words.append(\"Meghalaya\")\n",
        "    if(\"tripura\" in str1):\n",
        "      words.append(\"Tripura\")\n",
        "    if(\"mizoram\" in str1):\n",
        "      words.append(\"Mizoram\")  \n",
        "  elif(tag==\"equipment\"):\n",
        "    if(\"harvester\" in str1):\n",
        "      words.append(\"Harvester\")\n",
        "    if(\"sickle\" in str1):\n",
        "      words.append(\"Sickle\")\n",
        "    if(\"shoots\" in str1):\n",
        "      words.append(\"Shoots\")\n",
        "    if(\"wooden\" in str1):\n",
        "      words.append(\"Wooden Handled tools\")\n",
        "    if(\"plough\" in str1):\n",
        "      words.append(\"Plough\")\n",
        "    if(\"rototillers\" in str1):\n",
        "      words.append(\"Roto Tillers\")\n",
        "    if(\"transplantor\" in str1):\n",
        "      words.append(\"Transplantor\")\n",
        "    if(\"power\" in str1 and  \"weeder\" in str1):\n",
        "      words.append(\"Power Weeder\")\n",
        "    if(\"thresher\" in str1):\n",
        "      words.append(\"thresher\")      \n",
        "  elif(tag=='soilcondition'):\n",
        "    if(\"fertile\" in str1):\n",
        "      words.append(\"fertile\")\n",
        "    if(\"reverine\" in str1):\n",
        "      words.append(\"reverine\")\n",
        "    if(\"alluvial\" in str1):\n",
        "      words.append(\"alluvial\")\n",
        "    if(\"loam\" in str1):\n",
        "      words.append(\"loam\")\n",
        "    if(\"clay\" in str1):\n",
        "      words.append(\"clay\")\n",
        "    if(\"gravel\" in str1):\n",
        "      words.append(\"gravel\")\n",
        "    if(\"silts\" in str1):\n",
        "      words.append(\"gravel\")\n",
        "    if(\"sandy\" in str1):\n",
        "      words.append(\"sandy\")\n",
        "    if(\"red\" in str1):\n",
        "      words.append(\"red\")\n",
        "    if(\"black\" in str1):\n",
        "      words.append(\"black\")\n",
        "  elif(tag==\"fertilizers\"):\n",
        "    if(\"nitrogen\" in str1):\n",
        "      words.append(\"Nitrogen\")\n",
        "    if(\"pottasium\" in str1):\n",
        "      words.append(\"Pottasium\")\n",
        "    if(\"calcium\" in str1):\n",
        "      words.append(\"Calcium\")\n",
        "    if(\"nitrate\" in str1):\n",
        "      words.append(\"Nitrate\")\n",
        "    if(\"magnesium\" in str1):\n",
        "      words.append(\"Magnesium\")\n",
        "    if(\"ammonium\" in str1):\n",
        "      words.append(\"Ammonium\")\n",
        "    if(\"urea\" in str1):\n",
        "      words.append(\"Urea\")\n",
        "    if(\"sulphur\" in str1):\n",
        "      words.append(\"Sulphur\")\n",
        "    if(\"cow\" in str1 and \"dung\" in str1):\n",
        "      words.append(\"cow dung\")\n",
        "    if(\"green\" in str1 and \"leaves\" in str1):\n",
        "      words.append(\"green leaves\") \n",
        "    if(\"iron\" in str1):\n",
        "      words.append(\"Iron\")\n",
        "    if(\"zinc\" in str1):\n",
        "      words.append(\"zinc\")\n",
        "    if(\"phosphorous\" in str1):\n",
        "      words.append(\"phosphorous\")\n",
        "    if(\" n \" in str1):\n",
        "      words.append(\"N\")\n",
        "    if(\" p \" in str1):\n",
        "      words.append(\"P\")\n",
        "    if(\" k \" in str1):\n",
        "      words.append(\"K\")\n",
        "  elif(tag==\"weather\"):\n",
        "    if(\"hot\" in str1):\n",
        "      words.append(\"hot\")\n",
        "    if(\"humid\" in str1):\n",
        "      words.append(\"humid\")\n",
        "    if(\"cold\" in str1):\n",
        "      words.append(\"cold\")\n",
        "    if(\"warm\" in str1):\n",
        "      words.append(\"warm\")\n",
        "    if(\"temperate\" in str1):\n",
        "      words.append(\"temperate\")\n",
        "    if(\"dry\" in str1):\n",
        "      words.append(\"dry\")\n",
        "  elif(tag==\"attemp\"):\n",
        "    pattern = r'([+-]?\\d+(\\.\\d+)*)\\s?°([CcFf])'\n",
        "    l1=re.findall(pattern, str1)\n",
        "    for p in l1:\n",
        "      str2=p[0]+p[2]\n",
        "      words.append(str2)\n",
        "  # elif(tag==\"diseases\"):\n",
        "\n",
        "  return words\n",
        "\n",
        "\n",
        "def responsemessage(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # a random response from the intent\n",
        "                    # print(\"hi\");\n",
        "                    print (i['tag'])\n",
        "                    return i['tag'];\n",
        "                    # return;\n",
        "                    # break;\n",
        "                    # resp = (random.choice(i['responses']))\n",
        "                    # final_reply = entities.frommain(i['tag'],sentence)\n",
        "                    # # print rep\n",
        "                    # return final_reply"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZg-PM1e2goK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwSxBsDY7IgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dict1={\"Instate\":\"\",\"equipment\":\"\",\"soilcondition\":\"\",\"fertilizers\":\"\",\"weather\":\"\",\"attemp\":\"\",\"diseases\":\"\"}\n",
        "def main():\n",
        "    str = open('rice.txt', 'r').read()\n",
        "    str1=str.split('.')\n",
        "    print(len(str1))\n",
        "    Dict={\"Instate\":[],\"equipment\":[],\"soilcondition\":[],\"fertilizers\":[],\"weather\":[],\"attemp\":[],\"diseases\":[]}\n",
        "    for l in str1: \n",
        "      ser = responsemessage(l)\n",
        "      probabilities = classify(l)\n",
        "      print(probabilities)\n",
        "      if(ser):\n",
        "        Dict[ser]=Dict[ser]+get_words(l,ser)\n",
        "    print(Dict)\n",
        "    Dict[\"Instate\"]=list(set(Dict[\"Instate\"]))\n",
        "    str3=\"\"\n",
        "    size=len(Dict[\"Instate\"])\n",
        "    for i in range(0,size-1):\n",
        "      str3=str3+Dict[\"Instate\"][i]+\":\"\n",
        "    if(size>=1):  \n",
        "      str3=str3+Dict[\"Instate\"][size-1]\n",
        "    Dict1[\"Instate\"]=str3\n",
        "    Dict[\"equipment\"]=list(set(Dict[\"equipment\"]))\n",
        "    str3=\"\"\n",
        "    size=len(Dict[\"equipment\"])\n",
        "    for i in range(0,size-1):\n",
        "      str3=str3+Dict[\"equipment\"][i]+\":\"\n",
        "    if(size>=1):  \n",
        "      str3=str3+Dict[\"equipment\"][size-1]\n",
        "    Dict1[\"equipment\"]=str3\n",
        "    Dict[\"soilcondition\"]=list(set(Dict[\"soilcondition\"]))\n",
        "    str3=\"\"\n",
        "    size=len(Dict[\"soilcondition\"])\n",
        "    for i in range(0,size-1):\n",
        "      str3=str3+Dict[\"soilcondition\"][i]+\":\"\n",
        "    if(size>=1):  \n",
        "      str3=str3+Dict[\"soilcondition\"][size-1]\n",
        "    Dict1[\"soilcondition\"]=str3\n",
        "    Dict[\"fertilizers\"]=list(set(Dict[\"fertilizers\"]))\n",
        "    str3=\"\"\n",
        "    size=len(Dict[\"fertilizers\"])\n",
        "    for i in range(0,size-1):\n",
        "      str3=str3+Dict[\"fertilizers\"][i]+\":\"\n",
        "    if(size>=1):  \n",
        "      str3=str3+Dict[\"fertilizers\"][size-1]\n",
        "    Dict1[\"fertilizers\"]=str3\n",
        "    str3=\"\"\n",
        "    Dict[\"weather\"]=list(set(Dict[\"weather\"]))\n",
        "    size=len(Dict[\"weather\"])\n",
        "    for i in range(0,size-1):\n",
        "      str3=str3+Dict[\"weather\"][i]+\":\"\n",
        "    if(size>=1):  \n",
        "      str3=str3+Dict[\"weather\"][size-1]\n",
        "    Dict1[\"weather\"]=str3\n",
        "    str3=\"\"\n",
        "    Dict[\"attemp\"]=list(set(Dict[\"attemp\"]))\n",
        "    size=len(Dict[\"attemp\"])\n",
        "    for i in range(0,size-1):\n",
        "      str3=str3+Dict[\"attemp\"][i]+\":\"\n",
        "    if(size>=1):  \n",
        "      str3=str3+Dict[\"attemp\"][size-1]\n",
        "    Dict1[\"attemp\"]=str3\n",
        "    str3=\"\"\n",
        "    Dict[\"diseases\"]=list(set(Dict[\"diseases\"]))\n",
        "    size=len(Dict[\"diseases\"])\n",
        "    for i in range(0,size-1):\n",
        "      str3=str3+Dict[\"diseases\"][i]+\":\"\n",
        "    if(size>=1):\n",
        "      str3=str3+Dict[\"diseases\"][size-1]\n",
        "    Dict1[\"diseases\"]=str3\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVNPtESbS8gJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1=[]\n",
        "l1.append('Rice')\n",
        "l1.append(Dict1['Instate'])\n",
        "l1.append(Dict1['attemp'])\n",
        "l1.append(Dict1['soilcondition'])\n",
        "l1.append(Dict1['weather'])\n",
        "l1.append(Dict1['equipment'])\n",
        "l1.append(Dict1['fertilizers'])\n",
        "l1.append(Dict1['diseases'])\n",
        "from csv import writer\n",
        "with open(\"test.csv\", 'a+', newline='') as write_obj:\n",
        "  # Create a writer object from csv module\n",
        "  csv_writer = writer(write_obj)\n",
        "  # Add contents of list as last row in the csv file\n",
        "  csv_writer.writerow(l1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3l4FThAUfmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e340673d-1f32-4a5d-f4b1-1f716bf20f6d"
      },
      "source": [
        "print(l1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Rice', 'Uttar Pradesh:Punjab:Andhra Pradesh:West bengal', '35c:25c', 'red:loam:alluvial:fertile:gravel:black:sandy:clay', 'humid', 'Roto Tillers:Sickle:Wooden Handled tools', 'Nitrogen', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zucsCnkUw0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
